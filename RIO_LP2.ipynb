{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5dedfab",
   "metadata": {},
   "source": [
    "# REGRESSION ANALYSIS ON CORPORATION FAVORITA PRODUCTS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4159be0",
   "metadata": {},
   "source": [
    "## DESCRIPTION\n",
    "\n",
    "Corporation Favorita seeks to be informed  on the stocks of products it should have at a particular point in time by analysing the demand trend of all of its products by consumers by using machine learning model forcast. The regression analysis will use past sales data to identify patterns in demand and develop a model that can accurately forecast future demand. This model will then be used to inform purchasing and stocking decisions, reducing the likelihood of stockouts and overstocking. \n",
    "\n",
    "\n",
    "## OBJECTIVE\n",
    "\n",
    "The goal of this regression analysis is to optimize stock management at Corporation Favorita by accurately predicting demand for products in order to ensure that the right quantity of each product is always in stock. \n",
    "\n",
    "\n",
    "\n",
    "## HYPOTHESES\n",
    "\n",
    "# 1\n",
    "\n",
    "H0 - The type of day does not play a significant role in determining the demand for oil\n",
    "\n",
    "H1 - the type of day play a significant roles in determining the demand for oil\n",
    "\n",
    "\n",
    "# 2\n",
    "H0 - The location does not have an impact for the for the demand for oil\n",
    "\n",
    "H1 - The location have an impact for the demand for oil\n",
    "\n",
    "# 3\n",
    "\n",
    "H0 - There is no significant correlation between oil price and increase sales\n",
    "\n",
    "H1 - There is  significant correlation between oil price and increase sales\n",
    "\n",
    "\n",
    "## QUESTIONS\n",
    "\n",
    "1. Is the train dataset complete (has all the required dates)?\n",
    "\n",
    "2. Which dates have the lowest and highest sales for each year?\n",
    "\n",
    "4. Are certain groups of stores selling more products? (Cluster, city, state, type)\n",
    "\n",
    "5. Are sales affected by promotions, oil prices and holidays?\n",
    "\n",
    "7. What analysis can we get from the date and its extractable features?\n",
    "\n",
    "8. What is the difference between RMSLE, RMSE, MSE (or why is the MAE greater than all of them?)\n",
    "\n",
    "9.  What is the relationship between oil prices and sales?\n",
    "\n",
    "10. What is the relationship between product and sales?\n",
    "\n",
    "11. What is the trend of sales overtime ?\n",
    "\n",
    "12. What is the relationship between oil prices and promotion ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48e9766",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4472559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library for EDA\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64d7f7b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/Dell/Documents/PROJECT AZUBI/Career_Accelerator_LP2-Regression/sample_submission.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import datasets\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m df_sample_sub \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:/Users/Dell/Documents/PROJECT AZUBI/Career_Accelerator_LP2-Regression/sample_submission.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m df_stores \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/Dell/Documents/PROJECT AZUBI/Career_Accelerator_LP2-Regression/stores.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m df_trans \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/Dell/Documents/PROJECT AZUBI/Career_Accelerator_LP2-Regression/transactions.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/Dell/Documents/PROJECT AZUBI/Career_Accelerator_LP2-Regression/sample_submission.csv'"
     ]
    }
   ],
   "source": [
    "# Import datasets\n",
    "\n",
    "df_sample_sub = pd.read_csv('C:/Users/Dell/Documents/PROJECT AZUBI/Career_Accelerator_LP2-Regression/sample_submission.csv')\n",
    "df_stores = pd.read_csv('C:/Users/Dell/Documents/PROJECT AZUBI/Career_Accelerator_LP2-Regression/stores.csv')\n",
    "df_trans = pd.read_csv('C:/Users/Dell/Documents/PROJECT AZUBI/Career_Accelerator_LP2-Regression/transactions.csv')\n",
    "df_holi = pd.read_csv('C:/Users/Dell/Documents/PROJECT AZUBI/Career_Accelerator_LP2-Regression/holidays_events.csv')\n",
    "df_oil = pd.read_csv('C:/Users/Dell/Documents/PROJECT AZUBI/Career_Accelerator_LP2-Regression/oil.csv')\n",
    "\n",
    "#Loading train & test dataset\n",
    "df_train = pd.read_csv('C:/Users/Dell/Documents/PROJECT AZUBI/Career_Accelerator_LP2-Regression/train.csv')\n",
    "df_test = pd.read_csv('C:/Users/Dell/Documents/PROJECT AZUBI/Career_Accelerator_LP2-Regression/test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7512dd4",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8a2955",
   "metadata": {},
   "source": [
    "#### Sample Submission data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3908a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Submission data top 5\n",
    "df_sample_sub.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae53c909",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sample Submission data from bottom 5\n",
    "df_sample_sub.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9134fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Null values\n",
    "df_sample_sub.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150257bf",
   "metadata": {},
   "source": [
    "#### Stores data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17426e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The stores data - top 5\n",
    "df_stores.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1acee73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The stores data from bottom 5\n",
    "df_stores.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f4eff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Null values\n",
    "df_stores.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6f5b5a",
   "metadata": {},
   "source": [
    "#### Transactions data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6dd14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The transaction data - top 5\n",
    "df_trans.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c065d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The transactions data - bottom 5\n",
    "df_trans.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364f1225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Null values\n",
    "df_trans.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37877dcf",
   "metadata": {},
   "source": [
    "#### Holidays_events data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511dbd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holidays Event data - top 5\n",
    "df_holi.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd7e26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holidays Event data - bottom 5\n",
    "df_holi.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fff6e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Null values\n",
    "df_holi.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882bcff6",
   "metadata": {},
   "source": [
    "#### Oil data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97006948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The oil data - top 5\n",
    "df_oil.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea548c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The oil data - bottom 5\n",
    "df_oil.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8a38e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking Null values\n",
    "df_oil.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc0fa81",
   "metadata": {},
   "source": [
    "#### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd32621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Train data - top 5\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facc3125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Train data - bottom 5\n",
    "df_train.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a021e7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking Null values\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24004441",
   "metadata": {},
   "source": [
    "#### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd04e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Test data - top 5\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f43ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Test data - bottom 5\n",
    "df_test.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af852d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Null values\n",
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c129704b",
   "metadata": {},
   "source": [
    "### Checking for all data types all the data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eee719a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Submission data types\n",
    "df_sample_sub.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2322b1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores data types\n",
    "df_stores.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c790a7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction data types\n",
    "df_trans.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64759e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holidays event data types\n",
    "df_holi.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b407df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oil data types\n",
    "df_oil.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeed50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data types\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a3a0a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test data types\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4ce7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece9fef4",
   "metadata": {},
   "source": [
    "### Checking all the shape of all the data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef30624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking shapes for all datasets\n",
    "print(df_sample_sub.shape, df_stores.shape, df_trans.shape, df_oil.shape,df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573403fa",
   "metadata": {},
   "source": [
    "### Data Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe34cfe",
   "metadata": {},
   "source": [
    "\n",
    "1. 43 null values in  oil data before merge\n",
    "2. More missing values after merging \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43925773",
   "metadata": {},
   "source": [
    "### Solution to Issues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c404cc27",
   "metadata": {},
   "source": [
    "1. Use the simple imputer after merge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e22ec81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert date column to datetime format\n",
    "def to_dateTime(df):\n",
    "    # Convert 'date' column to datetime format\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# List of dataframes to convert\n",
    "dataframes = [df_trans, df_holi, df_oil, df_train, df_test]\n",
    "\n",
    "# Loop through dataframes and convert 'date' column to datetime format\n",
    "for df in dataframes:\n",
    "    to_dateTime(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72dcf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the datetime conversion on Train data\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc80eede",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7509e872",
   "metadata": {},
   "source": [
    "# Merge all datasets for further EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba669b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the datasets on common columns\n",
    "\n",
    "merged_data = pd.merge(df_train, df_trans, on=['date', 'store_nbr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699295b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Holiday data to previous merged data on date column\n",
    "merged_data2 = pd.merge(merged_data, df_holi, on='date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c24879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Oil data to previous merged data on date column\n",
    "merged_data3 = pd.merge(merged_data2, df_oil, on='date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae906571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Store data to previous merged data on store_nbr column\n",
    "\n",
    "merged_data4 = pd.merge(merged_data3, df_stores, on='store_nbr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697574cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview Merged data\n",
    "merged_data4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31449875",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rename columns using the rename method\n",
    "new_merged_data = merged_data4.rename(columns={\"type_x\": \"holiday_type\", \"type_y\": \"store_type\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e44ee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview of new merged data - top 10\n",
    "new_merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73492052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview of new merged data - bottom 10\n",
    "new_merged_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ab2105",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_merged_data['year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b594bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datatypes of new merged data\n",
    "new_merged_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebebf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect data for null values\n",
    "new_merged_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf94cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview of shape of new merged data\n",
    "new_merged_data.shape                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ee59bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display random sample of 5 rows\n",
    "new_merged_data.sample(5, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaa3c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New datatypes after changing date datatype as datetime\n",
    "new_merged_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308898e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Generate summary statistics for numerical columns in the DataFrame\n",
    "new_merged_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d117a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding duplicated valuew\n",
    "new_merged_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183522cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to CSV \n",
    "new_merged_data.to_csv('new_merged_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37358925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boxplot of the 'transactions' column grouped by 'locale'\n",
    "sns.boxplot(x='transactions', y='locale', data=new_merged_data)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c2a481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the boxplot using the Seaborn library \n",
    "sns.barplot(x='transactions', y='city', data=new_merged_data)\n",
    "\n",
    "\n",
    "width=0.5,  # Adjust the width of the boxes\n",
    "fliersize=3, # Adjust the size of the outliers\n",
    "showmeans=True, # Show the mean value\n",
    "meanline=True, # Show a line for the mean\n",
    "notch=True, # Make the boxes \"notched\"\n",
    "\n",
    "# Add a title and labels for the x and y axis\n",
    "plt.title(\"Transactions by City\", fontsize=18)\n",
    "plt.xlabel(\"Frequency\", fontsize=16)\n",
    "plt.ylabel(\"City\", fontsize=16)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04c5016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram of the 'transactions' column\n",
    "new_merged_data.transactions.hist()\n",
    "\n",
    "# Add labels to the x-axis, y-axis, and title\n",
    "plt.xlabel('Transactions', fontsize=16)\n",
    "plt.ylabel('Frequency', fontsize=16)\n",
    "plt.title('Histogram of Transactions', fontsize=20)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29515e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with numerical columns only\n",
    "numerical_df = new_merged_data.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# calculate the correlation matrix\n",
    "corr_matrix = numerical_df.corr()\n",
    "\n",
    "# display the correlation matrix\n",
    "print(corr_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab77b3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change date datatype as datetime to create new features\n",
    "\n",
    "new_merged_data.date = pd.to_datetime(new_merged_data.date)\n",
    "\n",
    "\n",
    "new_merged_data['year'] = new_merged_data.date.dt.year\n",
    "\n",
    "new_merged_data['month'] = new_merged_data.date.dt.month\n",
    "\n",
    "\n",
    "new_merged_data['dayofmonth'] = new_merged_data.date.dt.day\n",
    "\n",
    "\n",
    "new_merged_data['dayofweek'] = new_merged_data.date.dt.dayofweek\n",
    "\n",
    "\n",
    "new_merged_data['dayname'] = new_merged_data.date.dt.strftime('%A')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94097b5f",
   "metadata": {},
   "source": [
    "## Answering  Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d8701c",
   "metadata": {},
   "source": [
    "### 1. Is the train dataset complete (has all the required dates)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92be5f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "if df_train.isnull().values.any():\n",
    "  print(\"The dataset is not complete. There are missing values.\")\n",
    "\n",
    "# Check for missing dates in a time-series dataset\n",
    "if not df_train.index.is_unique:\n",
    "  print(\"The dataset is not complete. There are duplicate dates.\")\n",
    "else:\n",
    "  print(\"The dataset is complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3876bd",
   "metadata": {},
   "source": [
    "### 2. Which dates have the lowest and highest sales for each year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3357643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by year and get the minimum and maximum sales for each year\n",
    "grouped_by_year = new_merged_data.groupby(\"year\")[\"sales\"].agg([\"min\", \"max\"])\n",
    "\n",
    "# Get the dates corresponding to the minimum and maximum sales for each year\n",
    "result = pd.concat([new_merged_data[new_merged_data[\"sales\"] == grouped_by_year.loc[year, \"min\"]][[\"year\", \"date\"]].rename(columns={\"date\": \"date_min\"}) for year in grouped_by_year.index] +\n",
    "                  [new_merged_data[new_merged_data[\"sales\"] == grouped_by_year.loc[year, \"max\"]][[\"year\", \"date\"]].rename(columns={\"date\": \"date_max\"}) for year in grouped_by_year.index])\n",
    "\n",
    "# Set the index to be the year\n",
    "result = result.set_index(\"year\")\n",
    "\n",
    "# Group the data by year to get the minimum and maximum sales on separate rows\n",
    "result = result.groupby(level=0).agg({\"date_min\": \"first\", \"date_max\": \"first\"})\n",
    "\n",
    "# Reset the index to get a regular dataframe\n",
    "result = result.reset_index()\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b301a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by year and get the minimum and maximum sales for each year\n",
    "grouped_by_year = new_merged_data.groupby(\"year\")[\"sales\"].agg([\"min\", \"max\"])\n",
    "\n",
    "# Get the dates corresponding to the minimum and maximum sales for each year\n",
    "result = pd.concat([new_merged_data[new_merged_data[\"sales\"] == grouped_by_year.loc[year, \"min\"]][[\"year\", \"date\"]].rename(columns={\"date\": \"date_min\"}) for year in grouped_by_year.index] +\n",
    "                  [new_merged_data[new_merged_data[\"sales\"] == grouped_by_year.loc[year, \"max\"]][[\"year\", \"date\"]].rename(columns={\"date\": \"date_max\"}) for year in grouped_by_year.index])\n",
    "\n",
    "# Set the index to be the year\n",
    "result = result.set_index(\"year\")\n",
    "\n",
    "# Group the data by year to get the minimum and maximum sales on separate rows\n",
    "result = result.groupby(level=0).agg({\"date_min\": \"first\", \"date_max\": \"first\"})\n",
    "\n",
    "# Reset the index to get a regular dataframe\n",
    "result = result.reset_index()\n",
    "\n",
    "# Plot the minimum and maximum sales for each year\n",
    "plt.plot(result[\"year\"], grouped_by_year[\"min\"], label=\"Minimum Sales\")\n",
    "plt.plot(result[\"year\"], grouped_by_year[\"max\"], label=\"Maximum Sales\")\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Add axis labels\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Sales\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97d899a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5d22d91",
   "metadata": {},
   "source": [
    "### 3. Are certain groups of stores selling more products? (Cluster, city, state, type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c5c86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display random sample of 5 rows\n",
    "df_stores.sample(5, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6727eeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the number of stores by city\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(x='city', data=df_stores)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Number of Stores by City\")\n",
    "plt.xlabel(\"City\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"Number of Stores\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Plot the number of stores by state\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(x='state', data=df_stores)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Number of Stores by State\")\n",
    "plt.xlabel(\"State\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"Number of Stores\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Plot the number of stores by type\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(x='type', data=df_stores)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Number of Stores by Type\")\n",
    "plt.xlabel(\"Type\")\n",
    "plt.ylabel(\"Number of Stores\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Plot the number of stores by cluster\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(x='cluster', data=df_stores)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Number of Stores by Cluster\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Number of Stores\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79a4f66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f7f5059",
   "metadata": {},
   "source": [
    "### 5. What analysis can we get from the date and its extractable features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c61575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of the dataframe\n",
    "df_train_copy = df_train.copy()\n",
    "\n",
    "\n",
    "# extract year, quarter, month, day, and weekday information from the date column\n",
    "df_train_copy['year'] = df_train_copy['date'].dt.year\n",
    "df_train_copy['quarter'] = df_train_copy['date'].dt.quarter\n",
    "df_train_copy['month'] = df_train_copy['date'].dt.month\n",
    "df_train_copy['day'] = df_train_copy['date'].dt.day\n",
    "df_train_copy['weekday'] = df_train_copy['date'].dt.weekday\n",
    "\n",
    "# group sales data by year\n",
    "grouped_by_year = df_train_copy.groupby('year').sum()\n",
    "\n",
    "# plot the aggregated sales data by year\n",
    "plt.plot(grouped_by_year.index, grouped_by_year['sales'])\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.title(\"Sales by Year\")\n",
    "plt.show()\n",
    "\n",
    "# group sales data by month\n",
    "grouped_by_month = df_train_copy.groupby('month').sum()\n",
    "\n",
    "# plot the aggregated sales data by month\n",
    "plt.bar(grouped_by_month.index, grouped_by_month['sales'])\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.title(\"Sales by Month\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# group sales data by year\n",
    "grouped_by_quarter = df_train_copy.groupby('quarter').sum()\n",
    "\n",
    "# plot the aggregated sales data by quarter\n",
    "plt.plot(grouped_by_quarter.index, grouped_by_quarter['sales'])\n",
    "plt.xlabel(\"quarter\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.title(\"Sales by Quarter\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17308152",
   "metadata": {},
   "source": [
    "### 7. What is the relationship between oil prices and sales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1f02da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a scatter plot to visualize the relationship between oil prices and sales\n",
    "plt.scatter(new_merged_data['dcoilwtico'], new_merged_data['sales'])\n",
    "plt.xlabel('Oil Price')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('Relationship between Oil Prices and Sales')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318e51d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed5e5fc6",
   "metadata": {},
   "source": [
    "### 8. What is the relationship between product and sales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209cae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group data by product family and sum the sales\n",
    "grouped_data_1 = new_merged_data.groupby('family').sum()['sales']\n",
    "\n",
    "# Sort the data by sales\n",
    "grouped_data_1 = grouped_data_1.sort_values(ascending=False)\n",
    "\n",
    "# Plot the top 10 product families\n",
    "sns.barplot(x=grouped_data_1.index[:10], y=grouped_data_1.values[:10])\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Product Family')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('Relationship between Product Family and Sales (Top 10)')\n",
    "# Rotate the x-axis labels for better readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d236a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63e69aa1",
   "metadata": {},
   "source": [
    "### 9. What is the trend of sales overtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa68defd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group data by date and sum the sales\n",
    "date_group = new_merged_data.groupby(\"date\").sum()\n",
    "\n",
    "# Plot the sales over time\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(date_group.index, date_group[\"sales\"])\n",
    "plt.title(\"Sales Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bc8ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2f2e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45feda61",
   "metadata": {},
   "source": [
    "# Feature Processing & Engineering\n",
    "This section is to **clean**, **process** the dataset and **create new features**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a0bd55",
   "metadata": {},
   "source": [
    "## Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0454dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking duplicates in the train data\n",
    "new_merged_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ac47af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the specified columns\n",
    "new_merged_data = new_merged_data.drop(columns=[\"year\", \"month\", \"dayofmonth\", \"dayofweek\", \"dayname\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa428c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8de5b7e",
   "metadata": {},
   "source": [
    "## New Features Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d73870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change date datatype as datetime to create new features\n",
    "\n",
    "new_merged_data.date = pd.to_datetime(new_merged_data.date)\n",
    "\n",
    "\n",
    "new_merged_data['year'] = new_merged_data.date.dt.year\n",
    "\n",
    "new_merged_data['month'] = new_merged_data.date.dt.month\n",
    "\n",
    "\n",
    "new_merged_data['dayofmonth'] = new_merged_data.date.dt.day\n",
    "\n",
    "\n",
    "new_merged_data['dayofweek'] = new_merged_data.date.dt.dayofweek\n",
    "\n",
    "\n",
    "new_merged_data['dayname'] = new_merged_data.date.dt.strftime('%A')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe292cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview data with new features\n",
    "new_merged_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654dcd55",
   "metadata": {},
   "source": [
    "## Impute Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ffa5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# create an instance of the SimpleImputer class with mean strategy\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# fit the imputer to the dcoilwtico column of new_merged_data\n",
    "imputer.fit(new_merged_data[['dcoilwtico']])\n",
    "\n",
    "# use the imputer to transform the dcoilwtico column of new_merged_data, replacing missing values with the mean value\n",
    "new_merged_data['dcoilwtico'] = imputer.transform(new_merged_data[['dcoilwtico']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d798a28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview data columns after imputing\n",
    "new_merged_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6d0921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fd7292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrame to a CSV file\n",
    "new_merged_data.to_csv('new_merged_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cbc1a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#drop unnecessary columns\n",
    "final_data = new_merged_data.drop(columns=['id','locale', 'locale_name', 'description', 'transferred'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef83750",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdb7602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the date column as the index\n",
    "new_merged_data.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c8ac37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880c66dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop more columns\n",
    "\n",
    "final_data = new_merged_data.drop(columns=['state',  'store_type', 'dayname'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fddfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = new_merged_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f90b6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df02a5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorizing the products\n",
    "food_families = ['BEVERAGES', 'BREAD/BAKERY', 'FROZEN FOODS', 'MEATS', 'PREPARED FOODS', 'DELI','PRODUCE', 'DAIRY','POULTRY','EGGS','SEAFOOD']\n",
    "final_data['family'] = np.where(final_data['family'].isin(food_families), 'FOODS', final_data['family'])\n",
    "home_families = ['HOME AND KITCHEN I', 'HOME AND KITCHEN II', 'HOME APPLIANCES']\n",
    "final_data['family'] = np.where(final_data['family'].isin(home_families), 'HOME', final_data['family'])\n",
    "clothing_families = ['LINGERIE', 'LADYSWARE']\n",
    "final_data['family'] = np.where(final_data['family'].isin(clothing_families), 'CLOTHING', final_data['family'])\n",
    "grocery_families = ['GROCERY I', 'GROCERY II']\n",
    "final_data['family'] = np.where(final_data['family'].isin(grocery_families), 'GROCERY', final_data['family'])\n",
    "stationery_families = ['BOOKS', 'MAGAZINES','SCHOOL AND OFFICE SUPPLIES']\n",
    "final_data['family'] = np.where(final_data['family'].isin(stationery_families), 'STATIONERY', final_data['family'])\n",
    "cleaning_families = ['HOME CARE', 'BABY CARE','PERSONAL CARE']\n",
    "final_data['family'] = np.where(final_data['family'].isin(cleaning_families), 'CLEANING', final_data['family'])\n",
    "hardware_families = ['PLAYERS AND ELECTRONICS','HARDWARE']\n",
    "final_data['family'] = np.where(final_data['family'].isin(hardware_families), 'HARDWARE', final_data['family'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a053d931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783e8e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create an instance of StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# select numerical columns\n",
    "num_cols = ['sales', 'transactions', 'dcoilwtico', 'year', 'month', 'dayofmonth', 'dayofweek']\n",
    "\n",
    "# fit and transform the numerical columns\n",
    "final_data[num_cols] = scaler.fit_transform(final_data[num_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e14bb40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03a226eb",
   "metadata": {},
   "source": [
    "## Features Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce44b1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Select the categorical columns\n",
    "categorical_columns = [\"family\", \"city\", \"holiday_type\"]\n",
    "categorical_data = final_data[categorical_columns]\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Fit and transform the data to one hot encoding\n",
    "one_hot_encoded_data = encoder.fit_transform(categorical_data)\n",
    "\n",
    "# Get the categories for each column\n",
    "categories = [encoder.categories_[i] for i in range(len(encoder.categories_))]\n",
    "\n",
    "# Create the column names for the one hot encoded data\n",
    "column_names = []\n",
    "for i in range(len(categories)):\n",
    "    for j in range(len(categories[i])):\n",
    "        column_names.append(f'{categorical_columns[i]}_{categories[i][j]}')\n",
    "\n",
    "# Convert the one hot encoding data to a DataFrame\n",
    "one_hot_encoded_data = pd.DataFrame(one_hot_encoded_data.toarray(), columns=column_names)\n",
    "\n",
    "\n",
    "# Reset the index of both dataframes\n",
    "final_data = final_data.reset_index(drop=True)\n",
    "one_hot_encoded_data = one_hot_encoded_data.reset_index(drop=True)\n",
    "\n",
    "# Concatenate the original dataframe with the one hot encoded data\n",
    "final_data_encoded = pd.concat([final_data, one_hot_encoded_data], axis=1)\n",
    "\n",
    "# Drop the original categorical columns\n",
    "final_data_encoded.drop(categorical_columns, axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82383e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4a314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename dcoilwtico column to oil price\n",
    "final_data_encoded.rename(columns={'dcoilwtico':'oil_price'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519e4222",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d062886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the final_data_encoded as data\n",
    "data = final_data_encoded.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1624b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1137bcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 11))\n",
    "ax.plot(new_merged_data['sales'])\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Sales')\n",
    "fig.autofmt_xdate()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8cafdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrame to a CSV file\n",
    "data.to_csv('encoded_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aca0b25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79fc80f0",
   "metadata": {},
   "source": [
    "# Machine Learning Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2b9bb2",
   "metadata": {},
   "source": [
    "Here is the section to build, train, evaluate and compare the models to each others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059cd3b1",
   "metadata": {},
   "source": [
    "## Simple Model #001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db515ac",
   "metadata": {},
   "source": [
    "Please, keep the following structure to try all the model you want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afcc103",
   "metadata": {},
   "source": [
    "### Create and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9cfbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4901226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data to train and Test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create the feature dataframe using the selected columns\n",
    "X = data.drop([\"sales\"], axis=1)\n",
    "\n",
    "# Get the target variable\n",
    "y = data.sales\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8248fc02",
   "metadata": {},
   "source": [
    "### Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6011c9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Model\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction on X_test\n",
    "lr_predictions = lr.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6b3eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(y_test, lr_predictions)\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(\"Linear Regression\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c70c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics for Linear Regression\n",
    "lr_mse = mean_squared_error(y_test, lr_predictions).round(2)\n",
    "lr_rmse = np.sqrt(lr_mse).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88088413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the absolute value function to y_test to remove negative signs\n",
    "y_test_abs = abs(y_test)\n",
    "lr_predictions_abs = abs(lr_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab954692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean squared logarithmic error using the new y_test_abs and lr_predictions_abs array\n",
    "lr_rmsle = np.sqrt(mean_squared_log_error(y_test_abs, lr_predictions_abs)).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9c3141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the evaluation results for Linear Regression model\n",
    "print(\"\\nEvaluation Results for Linear Regression:\")\n",
    "print(\"MSE:\", lr_mse)\n",
    "print(\"RMSE:\", lr_rmse)\n",
    "print(\"RMSLE:\", lr_rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b683f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9fc8458",
   "metadata": {},
   "source": [
    "### Decision Tree Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f4832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Regression Model\n",
    "dt = DecisionTreeRegressor()\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction on X_test\n",
    "dt_predictions = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0109b881",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, dt_predictions)\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614493df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics for Decision Tree Regression\n",
    "dt_mse = mean_squared_error(y_test, dt_predictions).round(2)\n",
    "dt_rmse = np.sqrt(dt_mse).round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a36d91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the absolute value function to y_test to remove negative signs\n",
    "#y_test_abs = abs(y_test)\n",
    "dt_predictions_abs = abs(dt_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e818651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean squared logarithmic error using the new y_test_abs and dt_predictions_abs array\n",
    "\n",
    "dt_rmsle = np.sqrt(mean_squared_log_error(y_test_abs, dt_predictions_abs)).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3cf286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the evaluation results for Decision Tree Regression model\n",
    "print(\"\\nEvaluation Results for Decision Tree Regression:\")\n",
    "print(\"MSE:\", dt_mse)\n",
    "print(\"RMSE:\", dt_rmse)\n",
    "\n",
    "print(\"RMLSE:\", dt_rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13070e87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7931fbb1",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5de0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Model\n",
    "xgb = XGBRegressor()\n",
    "xgb.fit(X_train, y_train)\n",
    "xgb_predictions = xgb.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73703d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, xgb_predictions)\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(\"XGBoost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91eb33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics for XGBoost\n",
    "xgb_mse = mean_squared_error(y_test, xgb_predictions).round(2)\n",
    "xgb_rmse = np.sqrt(xgb_mse).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd3d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the absolute value function to y_test to remove negative signs\n",
    "#y_test_abs = abs(y_test)\n",
    "xgb_predictions_abs = abs(xgb_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f42cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean squared logarithmic error using the new y_test_abs and xgb_predictions_abs array\n",
    "\n",
    "xgb_rmsle = np.sqrt(mean_squared_log_error(y_test_abs, xgb_predictions_abs)).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb49604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the evaluation results for XGBoost model\n",
    "print(\"\\nEvaluation Results for XGBoost:\")\n",
    "print(\"MSE:\", xgb_mse)\n",
    "print(\"RMSE:\", xgb_rmse)\n",
    "print(\"RMSLE:\", xgb_rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e203329d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a660e312",
   "metadata": {},
   "source": [
    "### Random Forest Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb7b84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regression Model\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction on X_test\n",
    "rf_predictions = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadf49d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, rf_predictions)\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(\"Random Forest Regression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd84aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics for Random Forest Regression\n",
    "rf_mse = mean_squared_error(y_test, rf_predictions).round(2)\n",
    "rf_rmse = np.sqrt(rf_mse).round(2)\n",
    "#rf_rmsle = np.sqrt(mean_squared_error(np.log(y_test), np.log(rf_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4f041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the absolute value function to y_test to remove negative signs\n",
    "#y_test_abs = abs(y_test)\n",
    "rf_predictions_abs = abs(rf_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba80746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean squared logarithmic error using the new y_test_abs and rf_predictions_abs array\n",
    "\n",
    "rf_rmsle = np.sqrt(mean_squared_log_error(y_test_abs, rf_predictions_abs)).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8366d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the evaluation results for Random Forest Regrression model\n",
    "print(\"\\nEvaluation Results for Random Forest:\")\n",
    "print(\"MSE:\", rf_mse)\n",
    "print(\"RMSE:\", rf_rmse)\n",
    "print(\"RMSLE:\", rf_rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7ca3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2237448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table to compare the evaluation results\n",
    "results_table = pd.DataFrame({'Model': ['Linear Regression', 'Decision Tree', 'XGBoost', 'Random Forest'],\n",
    "                              'MSE': [lr_mse, dt_mse, xgb_mse, rf_mse],\n",
    "                              'RMSE': [lr_rmse, dt_rmse, xgb_rmse, rf_rmse],\n",
    "                              'RMSLE': [lr_rmsle, dt_rmsle, xgb_rmsle, rf_rmsle]})\n",
    "\n",
    "# Print the comparison table\n",
    "print(\"\\nComparison Table of Evaluation Results:\")\n",
    "print(results_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a14e0e06",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_merged_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnew_merged_data\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnew_merged_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'new_merged_data' is not defined"
     ]
    }
   ],
   "source": [
    "new_merged_data.to_csv('new_merged_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c360377c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
